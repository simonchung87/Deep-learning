{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timing\n",
    "Define function to count the runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-30T03:18:17.493440Z",
     "iopub.status.busy": "2020-11-30T03:18:17.492511Z",
     "iopub.status.idle": "2020-11-30T03:18:17.495464Z",
     "shell.execute_reply": "2020-11-30T03:18:17.494990Z"
    },
    "papermill": {
     "duration": 0.02044,
     "end_time": "2020-11-30T03:18:17.495582",
     "exception": false,
     "start_time": "2020-11-30T03:18:17.475142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "starttime = datetime.datetime.now()\n",
    "def running_hours(starttime):\n",
    "    return ((datetime.datetime.now()-starttime).seconds)/3600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-30T03:18:17.523793Z",
     "iopub.status.busy": "2020-11-30T03:18:17.522966Z",
     "iopub.status.idle": "2020-11-30T03:18:19.511592Z",
     "shell.execute_reply": "2020-11-30T03:18:19.510067Z"
    },
    "papermill": {
     "duration": 2.004476,
     "end_time": "2020-11-30T03:18:19.511711",
     "exception": false,
     "start_time": "2020-11-30T03:18:17.507235",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp -r ../input/earlystopping/early-stopping-pytorch-master/* ./\n",
    "from pytorchtools import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-11-30T03:18:19.543632Z",
     "iopub.status.busy": "2020-11-30T03:18:19.542849Z",
     "iopub.status.idle": "2020-11-30T03:18:20.702379Z",
     "shell.execute_reply": "2020-11-30T03:18:20.701190Z"
    },
    "papermill": {
     "duration": 1.179044,
     "end_time": "2020-11-30T03:18:20.702509",
     "exception": false,
     "start_time": "2020-11-30T03:18:19.523465",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import csv\n",
    "import random\n",
    "import matplotlib.image as img\n",
    "# import warnings\n",
    "import warnings\n",
    "# filter warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data import RandomSampler\n",
    "from torch.utils.data import TensorDataset\n",
    "from torchvision.utils import make_grid\n",
    "import torchvision.models as models\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Input Data Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-30T03:18:20.745168Z",
     "iopub.status.busy": "2020-11-30T03:18:20.742931Z",
     "iopub.status.idle": "2020-11-30T03:18:20.745990Z",
     "shell.execute_reply": "2020-11-30T03:18:20.746563Z"
    },
    "papermill": {
     "duration": 0.032196,
     "end_time": "2020-11-30T03:18:20.746696",
     "exception": false,
     "start_time": "2020-11-30T03:18:20.714500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Fashiondata(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_file, mode='train', transform=None):\n",
    "        \n",
    "        self.mode = mode   \n",
    "        self.data_list = [] \n",
    "        self.cate = []\n",
    "        self.attribute = []\n",
    "\n",
    "        self.transform = transform\n",
    "        \n",
    "        with open(csv_file, newline='') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            for row in reader:\n",
    "\n",
    "                self.data_list.append(\"../input/deep-fashion/\"+row['file_path'])\n",
    "                if mode != 'test': \n",
    "                    self.cate.append(row['category_label'])\n",
    "\n",
    "                    attr_temp = row['attribute_label'].split(\" \")\n",
    "                    attr_array = [0]*15\n",
    "                    for one in attr_temp:\n",
    "                        attr_array[int(one)] = 1\n",
    "                    self.attribute.append(attr_array)\n",
    "                    \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        data = Image.open(self.data_list[index])\n",
    "\n",
    "        if self.transform is not None:\n",
    "            data = self.transform(data)\n",
    "        if self.mode == 'test': \n",
    "            return data, self.data_list\n",
    "        cate = torch.tensor(int(self.cate[index]))\n",
    "        attr = torch.tensor(self.attribute[index], dtype = torch.float)\n",
    "        return data, cate, attr\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Data Argumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-30T03:18:20.782861Z",
     "iopub.status.busy": "2020-11-30T03:18:20.782105Z",
     "iopub.status.idle": "2020-11-30T03:18:20.788125Z",
     "shell.execute_reply": "2020-11-30T03:18:20.787585Z"
    },
    "papermill": {
     "duration": 0.02812,
     "end_time": "2020-11-30T03:18:20.788214",
     "exception": false,
     "start_time": "2020-11-30T03:18:20.760094",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "torch.manual_seed(20201126)\n",
    "transforms_train = transforms.Compose([\n",
    "  transforms.Resize((256, 256)),\n",
    "  transforms.RandomCrop((224, 224)),\n",
    "  transforms.RandomHorizontalFlip(),\n",
    "  transforms.RandomPerspective(),\n",
    "  transforms.RandomRotation(degrees = (0,90)),\n",
    "  transforms.ToTensor(), \n",
    "  transforms.Normalize(mean = (0.5, 0.5, 0.5), std = (0.5, 0.5, 0.5))\n",
    "\n",
    "])\n",
    "\n",
    "transforms_test = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.CenterCrop((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = (0.5, 0.5, 0.5), std = (0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-30T03:18:20.816924Z",
     "iopub.status.busy": "2020-11-30T03:18:20.816310Z",
     "iopub.status.idle": "2020-11-30T03:18:21.191121Z",
     "shell.execute_reply": "2020-11-30T03:18:21.189935Z"
    },
    "papermill": {
     "duration": 0.390836,
     "end_time": "2020-11-30T03:18:21.191264",
     "exception": false,
     "start_time": "2020-11-30T03:18:20.800428",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_train = Fashiondata('../input/deep-fashion/deep_fashion/train.csv', mode='train', transform=transforms_train)\n",
    "dataset_val = Fashiondata('../input/deep-fashion/deep_fashion/val.csv', mode='val', transform=transforms_test)\n",
    "dataset_test = Fashiondata('../input/deep-fashion/deep_fashion/test.csv', mode='test', transform=transforms_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-30T03:18:21.223216Z",
     "iopub.status.busy": "2020-11-30T03:18:21.221510Z",
     "iopub.status.idle": "2020-11-30T03:18:21.223885Z",
     "shell.execute_reply": "2020-11-30T03:18:21.224356Z"
    },
    "papermill": {
     "duration": 0.020693,
     "end_time": "2020-11-30T03:18:21.224482",
     "exception": false,
     "start_time": "2020-11-30T03:18:21.203789",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(dataset_val, batch_size=128, shuffle=False)\n",
    "test_loader = DataLoader(dataset_test, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tune Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-30T03:18:21.255905Z",
     "iopub.status.busy": "2020-11-30T03:18:21.254143Z",
     "iopub.status.idle": "2020-11-30T03:18:21.256536Z",
     "shell.execute_reply": "2020-11-30T03:18:21.257021Z"
    },
    "papermill": {
     "duration": 0.020333,
     "end_time": "2020-11-30T03:18:21.257131",
     "exception": false,
     "start_time": "2020-11-30T03:18:21.236798",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Neural Network Structure\n",
    "Used pretrained model (Resnet50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-30T03:18:21.311412Z",
     "iopub.status.busy": "2020-11-30T03:18:21.289832Z",
     "iopub.status.idle": "2020-11-30T03:18:44.623654Z",
     "shell.execute_reply": "2020-11-30T03:18:44.623098Z"
    },
    "papermill": {
     "duration": 23.354313,
     "end_time": "2020-11-30T03:18:44.623771",
     "exception": false,
     "start_time": "2020-11-30T03:18:21.269458",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f68743efd5a64e9b92a836bd20b983a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=102502400.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        model_ft = models.resnet50(pretrained=True)\n",
    "        #set_parameter_requires_grad(model_ft, True)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, 256)\n",
    "\n",
    "        self.pretrain_model = model_ft \n",
    "        self.category = nn.Sequential(\n",
    "            nn.Linear(256,256),\n",
    "            nn.Linear(256,10)\n",
    "        )\n",
    "        self.attr1 = nn.Sequential(\n",
    "            nn.Linear(256,256),\n",
    "            nn.Linear(256,1)\n",
    "        )\n",
    "        self.attr2 = nn.Sequential(\n",
    "            nn.Linear(256,256),\n",
    "            nn.Linear(256,1)\n",
    "        )\n",
    "        self.attr3 = nn.Sequential(\n",
    "            nn.Linear(256,256),\n",
    "            nn.Linear(256,1)\n",
    "        )\n",
    "        self.attr4 = nn.Sequential(\n",
    "            nn.Linear(256,256),\n",
    "            nn.Linear(256,1)\n",
    "        )\n",
    "        self.attr5 = nn.Sequential(\n",
    "            nn.Linear(256,256),\n",
    "            nn.Linear(256,1)\n",
    "        )\n",
    "        self.attr6 = nn.Sequential(\n",
    "            nn.Linear(256,256),\n",
    "            nn.Linear(256,1)\n",
    "        )\n",
    "        self.attr7 = nn.Sequential(\n",
    "            nn.Linear(256,256),\n",
    "            nn.Linear(256,1)\n",
    "        )\n",
    "        self.attr8 = nn.Sequential(\n",
    "            nn.Linear(256,256),\n",
    "            nn.Linear(256,1)\n",
    "        )\n",
    "        self.attr9 = nn.Sequential(\n",
    "            nn.Linear(256,256),\n",
    "            nn.Linear(256,1)\n",
    "        )\n",
    "        self.attr10 = nn.Sequential(\n",
    "            nn.Linear(256,256),\n",
    "            nn.Linear(256,1)\n",
    "        )\n",
    "        self.attr11 = nn.Sequential(\n",
    "            nn.Linear(256,256),\n",
    "            nn.Linear(256,1)\n",
    "        )\n",
    "        self.attr12 = nn.Sequential(\n",
    "            nn.Linear(256,256),\n",
    "            nn.Linear(256,1)\n",
    "        )\n",
    "        self.attr13 = nn.Sequential(\n",
    "            nn.Linear(256,256),\n",
    "            nn.Linear(256,1)\n",
    "        )\n",
    "        self.attr14 = nn.Sequential(\n",
    "            nn.Linear(256,256),\n",
    "            nn.Linear(256,1)\n",
    "        )\n",
    "        self.attr15 = nn.Sequential(\n",
    "            nn.Linear(256,256),\n",
    "            nn.Linear(256,1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.pretrain_model(x)\n",
    "        category = self.category(x)\n",
    "        attr1 = self.attr1(x)\n",
    "        attr2 = self.attr2(x)\n",
    "        attr3 = self.attr3(x)\n",
    "        attr4 = self.attr4(x)\n",
    "        attr5 = self.attr5(x)\n",
    "        attr6 = self.attr6(x)\n",
    "        attr7 = self.attr7(x)\n",
    "        attr8 = self.attr8(x)\n",
    "        attr9 = self.attr9(x)\n",
    "        attr10 = self.attr10(x)\n",
    "        attr11 = self.attr11(x)\n",
    "        attr12 = self.attr12(x)\n",
    "        attr13 = self.attr13(x)\n",
    "        attr14 = self.attr14(x)\n",
    "        attr15 = self.attr15(x)\n",
    "        \n",
    "        attribute_list = [attr1,attr2,attr3,attr4,attr5,attr6,attr7,attr8,attr9,attr10,attr11,attr12,attr13,attr14,attr15]\n",
    "        #attribute_list = torch.tensor([attr.to(\"cpu\").detach().numpy()  for attr in attribute_list],dtype = torch.float)\n",
    "        attribute_list = torch.stack(attribute_list)\n",
    "        attribute_list = torch.transpose(attribute_list, 0,1).reshape((-1,15,))\n",
    "        return category, attribute_list\n",
    "\n",
    "\n",
    "\n",
    "model = Net()\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-30T03:18:44.662254Z",
     "iopub.status.busy": "2020-11-30T03:18:44.658205Z",
     "iopub.status.idle": "2020-11-30T03:18:44.688070Z",
     "shell.execute_reply": "2020-11-30T03:18:44.689097Z"
    },
    "papermill": {
     "duration": 0.051044,
     "end_time": "2020-11-30T03:18:44.689261",
     "exception": false,
     "start_time": "2020-11-30T03:18:44.638217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t pretrain_model.conv1.weight\n",
      "\t pretrain_model.bn1.weight\n",
      "\t pretrain_model.bn1.bias\n",
      "\t pretrain_model.layer1.0.conv1.weight\n",
      "\t pretrain_model.layer1.0.bn1.weight\n",
      "\t pretrain_model.layer1.0.bn1.bias\n",
      "\t pretrain_model.layer1.0.conv2.weight\n",
      "\t pretrain_model.layer1.0.bn2.weight\n",
      "\t pretrain_model.layer1.0.bn2.bias\n",
      "\t pretrain_model.layer1.0.conv3.weight\n",
      "\t pretrain_model.layer1.0.bn3.weight\n",
      "\t pretrain_model.layer1.0.bn3.bias\n",
      "\t pretrain_model.layer1.0.downsample.0.weight\n",
      "\t pretrain_model.layer1.0.downsample.1.weight\n",
      "\t pretrain_model.layer1.0.downsample.1.bias\n",
      "\t pretrain_model.layer1.1.conv1.weight\n",
      "\t pretrain_model.layer1.1.bn1.weight\n",
      "\t pretrain_model.layer1.1.bn1.bias\n",
      "\t pretrain_model.layer1.1.conv2.weight\n",
      "\t pretrain_model.layer1.1.bn2.weight\n",
      "\t pretrain_model.layer1.1.bn2.bias\n",
      "\t pretrain_model.layer1.1.conv3.weight\n",
      "\t pretrain_model.layer1.1.bn3.weight\n",
      "\t pretrain_model.layer1.1.bn3.bias\n",
      "\t pretrain_model.layer1.2.conv1.weight\n",
      "\t pretrain_model.layer1.2.bn1.weight\n",
      "\t pretrain_model.layer1.2.bn1.bias\n",
      "\t pretrain_model.layer1.2.conv2.weight\n",
      "\t pretrain_model.layer1.2.bn2.weight\n",
      "\t pretrain_model.layer1.2.bn2.bias\n",
      "\t pretrain_model.layer1.2.conv3.weight\n",
      "\t pretrain_model.layer1.2.bn3.weight\n",
      "\t pretrain_model.layer1.2.bn3.bias\n",
      "\t pretrain_model.layer2.0.conv1.weight\n",
      "\t pretrain_model.layer2.0.bn1.weight\n",
      "\t pretrain_model.layer2.0.bn1.bias\n",
      "\t pretrain_model.layer2.0.conv2.weight\n",
      "\t pretrain_model.layer2.0.bn2.weight\n",
      "\t pretrain_model.layer2.0.bn2.bias\n",
      "\t pretrain_model.layer2.0.conv3.weight\n",
      "\t pretrain_model.layer2.0.bn3.weight\n",
      "\t pretrain_model.layer2.0.bn3.bias\n",
      "\t pretrain_model.layer2.0.downsample.0.weight\n",
      "\t pretrain_model.layer2.0.downsample.1.weight\n",
      "\t pretrain_model.layer2.0.downsample.1.bias\n",
      "\t pretrain_model.layer2.1.conv1.weight\n",
      "\t pretrain_model.layer2.1.bn1.weight\n",
      "\t pretrain_model.layer2.1.bn1.bias\n",
      "\t pretrain_model.layer2.1.conv2.weight\n",
      "\t pretrain_model.layer2.1.bn2.weight\n",
      "\t pretrain_model.layer2.1.bn2.bias\n",
      "\t pretrain_model.layer2.1.conv3.weight\n",
      "\t pretrain_model.layer2.1.bn3.weight\n",
      "\t pretrain_model.layer2.1.bn3.bias\n",
      "\t pretrain_model.layer2.2.conv1.weight\n",
      "\t pretrain_model.layer2.2.bn1.weight\n",
      "\t pretrain_model.layer2.2.bn1.bias\n",
      "\t pretrain_model.layer2.2.conv2.weight\n",
      "\t pretrain_model.layer2.2.bn2.weight\n",
      "\t pretrain_model.layer2.2.bn2.bias\n",
      "\t pretrain_model.layer2.2.conv3.weight\n",
      "\t pretrain_model.layer2.2.bn3.weight\n",
      "\t pretrain_model.layer2.2.bn3.bias\n",
      "\t pretrain_model.layer2.3.conv1.weight\n",
      "\t pretrain_model.layer2.3.bn1.weight\n",
      "\t pretrain_model.layer2.3.bn1.bias\n",
      "\t pretrain_model.layer2.3.conv2.weight\n",
      "\t pretrain_model.layer2.3.bn2.weight\n",
      "\t pretrain_model.layer2.3.bn2.bias\n",
      "\t pretrain_model.layer2.3.conv3.weight\n",
      "\t pretrain_model.layer2.3.bn3.weight\n",
      "\t pretrain_model.layer2.3.bn3.bias\n",
      "\t pretrain_model.layer3.0.conv1.weight\n",
      "\t pretrain_model.layer3.0.bn1.weight\n",
      "\t pretrain_model.layer3.0.bn1.bias\n",
      "\t pretrain_model.layer3.0.conv2.weight\n",
      "\t pretrain_model.layer3.0.bn2.weight\n",
      "\t pretrain_model.layer3.0.bn2.bias\n",
      "\t pretrain_model.layer3.0.conv3.weight\n",
      "\t pretrain_model.layer3.0.bn3.weight\n",
      "\t pretrain_model.layer3.0.bn3.bias\n",
      "\t pretrain_model.layer3.0.downsample.0.weight\n",
      "\t pretrain_model.layer3.0.downsample.1.weight\n",
      "\t pretrain_model.layer3.0.downsample.1.bias\n",
      "\t pretrain_model.layer3.1.conv1.weight\n",
      "\t pretrain_model.layer3.1.bn1.weight\n",
      "\t pretrain_model.layer3.1.bn1.bias\n",
      "\t pretrain_model.layer3.1.conv2.weight\n",
      "\t pretrain_model.layer3.1.bn2.weight\n",
      "\t pretrain_model.layer3.1.bn2.bias\n",
      "\t pretrain_model.layer3.1.conv3.weight\n",
      "\t pretrain_model.layer3.1.bn3.weight\n",
      "\t pretrain_model.layer3.1.bn3.bias\n",
      "\t pretrain_model.layer3.2.conv1.weight\n",
      "\t pretrain_model.layer3.2.bn1.weight\n",
      "\t pretrain_model.layer3.2.bn1.bias\n",
      "\t pretrain_model.layer3.2.conv2.weight\n",
      "\t pretrain_model.layer3.2.bn2.weight\n",
      "\t pretrain_model.layer3.2.bn2.bias\n",
      "\t pretrain_model.layer3.2.conv3.weight\n",
      "\t pretrain_model.layer3.2.bn3.weight\n",
      "\t pretrain_model.layer3.2.bn3.bias\n",
      "\t pretrain_model.layer3.3.conv1.weight\n",
      "\t pretrain_model.layer3.3.bn1.weight\n",
      "\t pretrain_model.layer3.3.bn1.bias\n",
      "\t pretrain_model.layer3.3.conv2.weight\n",
      "\t pretrain_model.layer3.3.bn2.weight\n",
      "\t pretrain_model.layer3.3.bn2.bias\n",
      "\t pretrain_model.layer3.3.conv3.weight\n",
      "\t pretrain_model.layer3.3.bn3.weight\n",
      "\t pretrain_model.layer3.3.bn3.bias\n",
      "\t pretrain_model.layer3.4.conv1.weight\n",
      "\t pretrain_model.layer3.4.bn1.weight\n",
      "\t pretrain_model.layer3.4.bn1.bias\n",
      "\t pretrain_model.layer3.4.conv2.weight\n",
      "\t pretrain_model.layer3.4.bn2.weight\n",
      "\t pretrain_model.layer3.4.bn2.bias\n",
      "\t pretrain_model.layer3.4.conv3.weight\n",
      "\t pretrain_model.layer3.4.bn3.weight\n",
      "\t pretrain_model.layer3.4.bn3.bias\n",
      "\t pretrain_model.layer3.5.conv1.weight\n",
      "\t pretrain_model.layer3.5.bn1.weight\n",
      "\t pretrain_model.layer3.5.bn1.bias\n",
      "\t pretrain_model.layer3.5.conv2.weight\n",
      "\t pretrain_model.layer3.5.bn2.weight\n",
      "\t pretrain_model.layer3.5.bn2.bias\n",
      "\t pretrain_model.layer3.5.conv3.weight\n",
      "\t pretrain_model.layer3.5.bn3.weight\n",
      "\t pretrain_model.layer3.5.bn3.bias\n",
      "\t pretrain_model.layer4.0.conv1.weight\n",
      "\t pretrain_model.layer4.0.bn1.weight\n",
      "\t pretrain_model.layer4.0.bn1.bias\n",
      "\t pretrain_model.layer4.0.conv2.weight\n",
      "\t pretrain_model.layer4.0.bn2.weight\n",
      "\t pretrain_model.layer4.0.bn2.bias\n",
      "\t pretrain_model.layer4.0.conv3.weight\n",
      "\t pretrain_model.layer4.0.bn3.weight\n",
      "\t pretrain_model.layer4.0.bn3.bias\n",
      "\t pretrain_model.layer4.0.downsample.0.weight\n",
      "\t pretrain_model.layer4.0.downsample.1.weight\n",
      "\t pretrain_model.layer4.0.downsample.1.bias\n",
      "\t pretrain_model.layer4.1.conv1.weight\n",
      "\t pretrain_model.layer4.1.bn1.weight\n",
      "\t pretrain_model.layer4.1.bn1.bias\n",
      "\t pretrain_model.layer4.1.conv2.weight\n",
      "\t pretrain_model.layer4.1.bn2.weight\n",
      "\t pretrain_model.layer4.1.bn2.bias\n",
      "\t pretrain_model.layer4.1.conv3.weight\n",
      "\t pretrain_model.layer4.1.bn3.weight\n",
      "\t pretrain_model.layer4.1.bn3.bias\n",
      "\t pretrain_model.layer4.2.conv1.weight\n",
      "\t pretrain_model.layer4.2.bn1.weight\n",
      "\t pretrain_model.layer4.2.bn1.bias\n",
      "\t pretrain_model.layer4.2.conv2.weight\n",
      "\t pretrain_model.layer4.2.bn2.weight\n",
      "\t pretrain_model.layer4.2.bn2.bias\n",
      "\t pretrain_model.layer4.2.conv3.weight\n",
      "\t pretrain_model.layer4.2.bn3.weight\n",
      "\t pretrain_model.layer4.2.bn3.bias\n",
      "\t pretrain_model.fc.weight\n",
      "\t pretrain_model.fc.bias\n",
      "\t category.0.weight\n",
      "\t category.0.bias\n",
      "\t category.1.weight\n",
      "\t category.1.bias\n",
      "\t attr1.0.weight\n",
      "\t attr1.0.bias\n",
      "\t attr1.1.weight\n",
      "\t attr1.1.bias\n",
      "\t attr2.0.weight\n",
      "\t attr2.0.bias\n",
      "\t attr2.1.weight\n",
      "\t attr2.1.bias\n",
      "\t attr3.0.weight\n",
      "\t attr3.0.bias\n",
      "\t attr3.1.weight\n",
      "\t attr3.1.bias\n",
      "\t attr4.0.weight\n",
      "\t attr4.0.bias\n",
      "\t attr4.1.weight\n",
      "\t attr4.1.bias\n",
      "\t attr5.0.weight\n",
      "\t attr5.0.bias\n",
      "\t attr5.1.weight\n",
      "\t attr5.1.bias\n",
      "\t attr6.0.weight\n",
      "\t attr6.0.bias\n",
      "\t attr6.1.weight\n",
      "\t attr6.1.bias\n",
      "\t attr7.0.weight\n",
      "\t attr7.0.bias\n",
      "\t attr7.1.weight\n",
      "\t attr7.1.bias\n",
      "\t attr8.0.weight\n",
      "\t attr8.0.bias\n",
      "\t attr8.1.weight\n",
      "\t attr8.1.bias\n",
      "\t attr9.0.weight\n",
      "\t attr9.0.bias\n",
      "\t attr9.1.weight\n",
      "\t attr9.1.bias\n",
      "\t attr10.0.weight\n",
      "\t attr10.0.bias\n",
      "\t attr10.1.weight\n",
      "\t attr10.1.bias\n",
      "\t attr11.0.weight\n",
      "\t attr11.0.bias\n",
      "\t attr11.1.weight\n",
      "\t attr11.1.bias\n",
      "\t attr12.0.weight\n",
      "\t attr12.0.bias\n",
      "\t attr12.1.weight\n",
      "\t attr12.1.bias\n",
      "\t attr13.0.weight\n",
      "\t attr13.0.bias\n",
      "\t attr13.1.weight\n",
      "\t attr13.1.bias\n",
      "\t attr14.0.weight\n",
      "\t attr14.0.bias\n",
      "\t attr14.1.weight\n",
      "\t attr14.1.bias\n",
      "\t attr15.0.weight\n",
      "\t attr15.0.bias\n",
      "\t attr15.1.weight\n",
      "\t attr15.1.bias\n"
     ]
    }
   ],
   "source": [
    "params_to_update = model.parameters()\n",
    "print(\"Params to learn:\")\n",
    "params_to_update = []\n",
    "for name,param in model.named_parameters():\n",
    "  if param.requires_grad == True:\n",
    "    params_to_update.append(param)\n",
    "    print(\"\\t\",name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define criterions and optimizer\n",
    "The first Criterion for multi-label classification is Cross Entropy Loss\n",
    "\n",
    "The Second Criterion for multi-class classification is Binary Cross Entropy loss\n",
    "\n",
    "Optimizer: Adam with learning rate 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-30T03:18:44.747322Z",
     "iopub.status.busy": "2020-11-30T03:18:44.746271Z",
     "iopub.status.idle": "2020-11-30T03:18:44.760941Z",
     "shell.execute_reply": "2020-11-30T03:18:44.761494Z"
    },
    "papermill": {
     "duration": 0.050197,
     "end_time": "2020-11-30T03:18:44.761627",
     "exception": false,
     "start_time": "2020-11-30T03:18:44.711430",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion1, criterion2 = nn.CrossEntropyLoss(), nn.BCEWithLogitsLoss()\n",
    "#optimizer = torch.optim.SGD(params=model.parameters(), lr=0.01, momentum=0.9) # throw param into optimizer some_optimier(param, lr=...)\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr=0.01, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "#optimizer = torch.optim.AdamW(params = model.parameters(), lr=0.01, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)\n",
    "criterion1, criterion2 = criterion1.cuda(), criterion2.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define training and validation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-30T03:18:44.809388Z",
     "iopub.status.busy": "2020-11-30T03:18:44.808697Z",
     "iopub.status.idle": "2020-11-30T03:18:44.900745Z",
     "shell.execute_reply": "2020-11-30T03:18:44.900059Z"
    },
    "papermill": {
     "duration": 0.122477,
     "end_time": "2020-11-30T03:18:44.900847",
     "exception": false,
     "start_time": "2020-11-30T03:18:44.778370",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def train(input_data, model, criterion1, criterion2, optimizer):\n",
    "    model.train()\n",
    "    loss_list = []\n",
    "    all_predict_attr = torch.tensor([]).cuda()\n",
    "    all_true_attr = torch.tensor([]).cuda()\n",
    "    total_count = 0\n",
    "    acc_count = 0\n",
    "    for i, data in enumerate(input_data, 0):\n",
    "        images, cate, attr = data[0].cuda(), data[1].cuda(), data[2].cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output_cate, output_attr = model(images)\n",
    "        output_attr = output_attr.cuda()\n",
    "        loss1 = criterion1(output_cate, cate)\n",
    "        loss2 = criterion2(output_attr, attr)\n",
    "        loss = loss1 + loss2 * 4.38\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        \n",
    "        _, predict_cate = torch.max(output_cate.data, 1)\n",
    "        predict_attr = torch.tensor(output_attr > 0, dtype = torch.int).cuda()\n",
    "        all_predict_attr = torch.cat((all_predict_attr, predict_attr), 0)\n",
    "        all_true_attr = torch.cat((all_true_attr, attr), 0)\n",
    "        total_count += cate.size(0) \n",
    "        acc_count += (predict_cate == cate).sum()\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "    all_predict_attr = all_predict_attr.reshape((-1,15))\n",
    "    all_true_attr = all_true_attr.reshape((-1,15))\n",
    "    attr_f1 = f1_score(all_predict_attr.to(\"cpu\"), all_true_attr.to(\"cpu\"), average='samples')\n",
    "    acc = acc_count.to(\"cpu\").detach().numpy() / total_count\n",
    "    loss = sum(loss_list) / len(loss_list)\n",
    "    return acc, loss, attr_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-30T03:18:44.947877Z",
     "iopub.status.busy": "2020-11-30T03:18:44.944805Z",
     "iopub.status.idle": "2020-11-30T03:18:44.950494Z",
     "shell.execute_reply": "2020-11-30T03:18:44.950006Z"
    },
    "papermill": {
     "duration": 0.034153,
     "end_time": "2020-11-30T03:18:44.950606",
     "exception": false,
     "start_time": "2020-11-30T03:18:44.916453",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def val(input_data, model, criterion1, criterion2):\n",
    "    model.eval()\n",
    "    \n",
    "    loss_list = []\n",
    "    all_predict_attr = torch.tensor([]).cuda()\n",
    "    all_true_attr = torch.tensor([]).cuda()\n",
    "    total_count = 0\n",
    "    acc_count = 0\n",
    "    with torch.no_grad():\n",
    "        for data in input_data:\n",
    "            images, cate, attr = data[0].cuda(), data[1].cuda(), data[2].cuda()\n",
    "            output_cate, output_attr = model(images)\n",
    "            output_attr = output_attr.cuda()\n",
    "            loss1 = criterion1(output_cate, cate)\n",
    "            loss2 = criterion2(output_attr.cuda(), attr)\n",
    "            loss = loss1 + loss2 * 4.38\n",
    "\n",
    "            _, predict_cate = torch.max(output_cate.data, 1)\n",
    "            predict_attr = torch.tensor(output_attr > 0, dtype = torch.int).cuda()\n",
    "            all_predict_attr = torch.cat((all_predict_attr, predict_attr), 0)\n",
    "            all_true_attr = torch.cat((all_true_attr, attr), 0)\n",
    "            total_count += cate.size(0) \n",
    "            acc_count += (predict_cate == cate).sum()\n",
    "            loss_list.append(loss.item())\n",
    "    all_predict_attr = all_predict_attr.reshape((-1,15))\n",
    "    all_true_attr = all_true_attr.reshape((-1,15))\n",
    "    attr_f1 = f1_score(all_predict_attr.to(\"cpu\"), all_true_attr.to(\"cpu\"), average='samples')\n",
    "    acc = acc_count.to(\"cpu\").detach().numpy() / total_count\n",
    "    loss = sum(loss_list) / len(loss_list)\n",
    "    return acc, loss, attr_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training\n",
    "Maximum epoch: 100\n",
    "\n",
    "Early Stopping patience: 10\n",
    "\n",
    "Save two models seperatively for different task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-30T03:18:44.996035Z",
     "iopub.status.busy": "2020-11-30T03:18:44.995197Z",
     "iopub.status.idle": "2020-11-30T11:54:54.497606Z",
     "shell.execute_reply": "2020-11-30T11:54:54.498379Z"
    },
    "papermill": {
     "duration": 30969.532187,
     "end_time": "2020-11-30T11:54:54.498613",
     "exception": false,
     "start_time": "2020-11-30T03:18:44.966426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Epoch 1 ====================\n",
      "Running hours: 0.16\n",
      "Train Acc: 0.117876 Train Loss: 4.474619 Train F1: 0.005921\n",
      "  Val Acc: 0.111614   Val Loss: 3.696094 Val F1: 0.007966\n",
      "Validation loss decreased (inf --> 0.888386).  Saving model ...\n",
      "Validation loss decreased (inf --> 0.992034).  Saving model ...\n",
      "==================== Epoch 2 ====================\n",
      "Running hours: 0.29\n",
      "Train Acc: 0.121534 Train Loss: 3.622714 Train F1: 0.001068\n",
      "  Val Acc: 0.129522   Val Loss: 3.606134 Val F1: 0.006579\n",
      "Validation loss decreased (0.888386 --> 0.870478).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "==================== Epoch 3 ====================\n",
      "Running hours: 0.43\n",
      "Train Acc: 0.142280 Train Loss: 3.579292 Train F1: 0.002410\n",
      "  Val Acc: 0.147250   Val Loss: 3.536761 Val F1: 0.000543\n",
      "Validation loss decreased (0.870478 --> 0.852750).  Saving model ...\n",
      "EarlyStopping counter: 2 out of 10\n",
      "==================== Epoch 4 ====================\n",
      "Running hours: 0.56\n",
      "Train Acc: 0.159623 Train Loss: 3.524570 Train F1: 0.006274\n",
      "  Val Acc: 0.174566   Val Loss: 3.464736 Val F1: 0.023420\n",
      "Validation loss decreased (0.852750 --> 0.825434).  Saving model ...\n",
      "Validation loss decreased (0.992034 --> 0.976580).  Saving model ...\n",
      "==================== Epoch 5 ====================\n",
      "Running hours: 0.69\n",
      "Train Acc: 0.190934 Train Loss: 3.453788 Train F1: 0.017658\n",
      "  Val Acc: 0.188495   Val Loss: 3.477632 Val F1: 0.013043\n",
      "Validation loss decreased (0.825434 --> 0.811505).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "==================== Epoch 6 ====================\n",
      "Running hours: 0.83\n",
      "Train Acc: 0.234396 Train Loss: 3.350909 Train F1: 0.024123\n",
      "  Val Acc: 0.238423   Val Loss: 3.363547 Val F1: 0.029299\n",
      "Validation loss decreased (0.811505 --> 0.761577).  Saving model ...\n",
      "Validation loss decreased (0.976580 --> 0.970701).  Saving model ...\n",
      "==================== Epoch 7 ====================\n",
      "Running hours: 0.96\n",
      "Train Acc: 0.262816 Train Loss: 3.274355 Train F1: 0.040967\n",
      "  Val Acc: 0.198082   Val Loss: 3.616429 Val F1: 0.094201\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.970701 --> 0.905799).  Saving model ...\n",
      "==================== Epoch 8 ====================\n",
      "Running hours: 1.11\n",
      "Train Acc: 0.281285 Train Loss: 3.209885 Train F1: 0.060327\n",
      "  Val Acc: 0.222685   Val Loss: 3.890880 Val F1: 0.045288\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 1 out of 10\n",
      "==================== Epoch 9 ====================\n",
      "Running hours: 1.24\n",
      "Train Acc: 0.302901 Train Loss: 3.135660 Train F1: 0.071633\n",
      "  Val Acc: 0.260130   Val Loss: 3.496803 Val F1: 0.083192\n",
      "Validation loss decreased (0.761577 --> 0.739870).  Saving model ...\n",
      "EarlyStopping counter: 2 out of 10\n",
      "==================== Epoch 10 ====================\n",
      "Running hours: 1.37\n",
      "Train Acc: 0.317661 Train Loss: 3.085430 Train F1: 0.078633\n",
      "  Val Acc: 0.259588   Val Loss: 3.344871 Val F1: 0.059284\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "==================== Epoch 11 ====================\n",
      "Running hours: 1.51\n",
      "Train Acc: 0.332805 Train Loss: 3.038522 Train F1: 0.085241\n",
      "  Val Acc: 0.242764   Val Loss: 3.609720 Val F1: 0.150755\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Validation loss decreased (0.905799 --> 0.849245).  Saving model ...\n",
      "==================== Epoch 12 ====================\n",
      "Running hours: 1.64\n",
      "Train Acc: 0.350865 Train Loss: 2.986786 Train F1: 0.092282\n",
      "  Val Acc: 0.283647   Val Loss: 3.320221 Val F1: 0.153451\n",
      "Validation loss decreased (0.739870 --> 0.716353).  Saving model ...\n",
      "Validation loss decreased (0.849245 --> 0.846549).  Saving model ...\n",
      "==================== Epoch 13 ====================\n",
      "Running hours: 1.77\n",
      "Train Acc: 0.367262 Train Loss: 2.948266 Train F1: 0.096989\n",
      "  Val Acc: 0.307525   Val Loss: 3.108116 Val F1: 0.064442\n",
      "Validation loss decreased (0.716353 --> 0.692475).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "==================== Epoch 14 ====================\n",
      "Running hours: 1.91\n",
      "Train Acc: 0.377929 Train Loss: 2.910736 Train F1: 0.102902\n",
      "  Val Acc: 0.311686   Val Loss: 3.157705 Val F1: 0.089699\n",
      "Validation loss decreased (0.692475 --> 0.688314).  Saving model ...\n",
      "EarlyStopping counter: 2 out of 10\n",
      "==================== Epoch 15 ====================\n",
      "Running hours: 2.04\n",
      "Train Acc: 0.391129 Train Loss: 2.876594 Train F1: 0.113233\n",
      "  Val Acc: 0.329957   Val Loss: 3.025348 Val F1: 0.092683\n",
      "Validation loss decreased (0.688314 --> 0.670043).  Saving model ...\n",
      "EarlyStopping counter: 3 out of 10\n",
      "==================== Epoch 16 ====================\n",
      "Running hours: 2.17\n",
      "Train Acc: 0.400619 Train Loss: 2.846847 Train F1: 0.120425\n",
      "  Val Acc: 0.302460   Val Loss: 3.245100 Val F1: 0.123028\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "==================== Epoch 17 ====================\n",
      "Running hours: 2.31\n",
      "Train Acc: 0.412156 Train Loss: 2.813106 Train F1: 0.121239\n",
      "  Val Acc: 0.315304   Val Loss: 3.159377 Val F1: 0.103080\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 5 out of 10\n",
      "==================== Epoch 18 ====================\n",
      "Running hours: 2.44\n",
      "Train Acc: 0.401156 Train Loss: 2.862958 Train F1: 0.108155\n",
      "  Val Acc: 0.331223   Val Loss: 3.080942 Val F1: 0.065418\n",
      "Validation loss decreased (0.670043 --> 0.668777).  Saving model ...\n",
      "EarlyStopping counter: 6 out of 10\n",
      "==================== Epoch 19 ====================\n",
      "Running hours: 2.58\n",
      "Train Acc: 0.420546 Train Loss: 2.789610 Train F1: 0.127136\n",
      "  Val Acc: 0.329233   Val Loss: 3.128455 Val F1: 0.110207\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 7 out of 10\n",
      "==================== Epoch 20 ====================\n",
      "Running hours: 2.71\n",
      "Train Acc: 0.426072 Train Loss: 2.761872 Train F1: 0.137056\n",
      "  Val Acc: 0.341172   Val Loss: 3.040245 Val F1: 0.096219\n",
      "Validation loss decreased (0.668777 --> 0.658828).  Saving model ...\n",
      "EarlyStopping counter: 8 out of 10\n",
      "==================== Epoch 21 ====================\n",
      "Running hours: 2.85\n",
      "Train Acc: 0.433516 Train Loss: 2.747574 Train F1: 0.137990\n",
      "  Val Acc: 0.330861   Val Loss: 3.144144 Val F1: 0.078726\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 9 out of 10\n",
      "==================== Epoch 22 ====================\n",
      "Running hours: 2.98\n",
      "Train Acc: 0.442725 Train Loss: 2.715135 Train F1: 0.145994\n",
      "  Val Acc: 0.348227   Val Loss: 3.101868 Val F1: 0.120736\n",
      "Validation loss decreased (0.658828 --> 0.651773).  Saving model ...\n",
      "EarlyStopping counter: 10 out of 10\n",
      "==================== Epoch 23 ====================\n",
      "Running hours: 3.12\n",
      "Train Acc: 0.444413 Train Loss: 2.701216 Train F1: 0.148438\n",
      "  Val Acc: 0.317656   Val Loss: 3.183067 Val F1: 0.151187\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 11 out of 10\n",
      "==================== Epoch 24 ====================\n",
      "Running hours: 3.25\n",
      "Train Acc: 0.456129 Train Loss: 2.673075 Train F1: 0.155805\n",
      "  Val Acc: 0.359624   Val Loss: 3.020148 Val F1: 0.154388\n",
      "Validation loss decreased (0.651773 --> 0.640376).  Saving model ...\n",
      "Validation loss decreased (0.846549 --> 0.845612).  Saving model ...\n",
      "==================== Epoch 25 ====================\n",
      "Running hours: 3.38\n",
      "Train Acc: 0.461782 Train Loss: 2.656202 Train F1: 0.156587\n",
      "  Val Acc: 0.357996   Val Loss: 3.028197 Val F1: 0.138478\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 1 out of 10\n",
      "==================== Epoch 26 ====================\n",
      "Running hours: 3.52\n",
      "Train Acc: 0.465850 Train Loss: 2.643505 Train F1: 0.158694\n",
      "  Val Acc: 0.394718   Val Loss: 2.851837 Val F1: 0.145217\n",
      "Validation loss decreased (0.640376 --> 0.605282).  Saving model ...\n",
      "EarlyStopping counter: 2 out of 10\n",
      "==================== Epoch 27 ====================\n",
      "Running hours: 3.66\n",
      "Train Acc: 0.472910 Train Loss: 2.623056 Train F1: 0.164852\n",
      "  Val Acc: 0.332127   Val Loss: 3.178273 Val F1: 0.135751\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "==================== Epoch 28 ====================\n",
      "Running hours: 3.80\n",
      "Train Acc: 0.480456 Train Loss: 2.600891 Train F1: 0.169604\n",
      "  Val Acc: 0.323625   Val Loss: 3.088780 Val F1: 0.125084\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "==================== Epoch 29 ====================\n",
      "Running hours: 3.93\n",
      "Train Acc: 0.482273 Train Loss: 2.591251 Train F1: 0.174331\n",
      "  Val Acc: 0.398517   Val Loss: 2.834180 Val F1: 0.139218\n",
      "Validation loss decreased (0.605282 --> 0.601483).  Saving model ...\n",
      "EarlyStopping counter: 5 out of 10\n",
      "==================== Epoch 30 ====================\n",
      "Running hours: 4.07\n",
      "Train Acc: 0.485803 Train Loss: 2.575383 Train F1: 0.171638\n",
      "  Val Acc: 0.382236   Val Loss: 2.907251 Val F1: 0.120740\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 6 out of 10\n",
      "==================== Epoch 31 ====================\n",
      "Running hours: 4.20\n",
      "Train Acc: 0.495677 Train Loss: 2.551052 Train F1: 0.183333\n",
      "  Val Acc: 0.397250   Val Loss: 2.877759 Val F1: 0.152676\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 7 out of 10\n",
      "==================== Epoch 32 ====================\n",
      "Running hours: 4.34\n",
      "Train Acc: 0.502046 Train Loss: 2.528265 Train F1: 0.186477\n",
      "  Val Acc: 0.384226   Val Loss: 2.935181 Val F1: 0.126049\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 8 out of 10\n",
      "==================== Epoch 33 ====================\n",
      "Running hours: 4.47\n",
      "Train Acc: 0.500998 Train Loss: 2.524779 Train F1: 0.187442\n",
      "  Val Acc: 0.375724   Val Loss: 2.988546 Val F1: 0.124355\n",
      "EarlyStopping counter: 4 out of 10\n",
      "EarlyStopping counter: 9 out of 10\n",
      "==================== Epoch 34 ====================\n",
      "Running hours: 4.60\n",
      "Train Acc: 0.506651 Train Loss: 2.504409 Train F1: 0.193123\n",
      "  Val Acc: 0.391281   Val Loss: 2.931023 Val F1: 0.163557\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Validation loss decreased (0.845612 --> 0.836443).  Saving model ...\n",
      "==================== Epoch 35 ====================\n",
      "Running hours: 4.74\n",
      "Train Acc: 0.510053 Train Loss: 2.497004 Train F1: 0.195335\n",
      "  Val Acc: 0.322178   Val Loss: 3.448241 Val F1: 0.129421\n",
      "EarlyStopping counter: 6 out of 10\n",
      "EarlyStopping counter: 1 out of 10\n",
      "==================== Epoch 36 ====================\n",
      "Running hours: 4.87\n",
      "Train Acc: 0.515067 Train Loss: 2.479355 Train F1: 0.201256\n",
      "  Val Acc: 0.395260   Val Loss: 2.916447 Val F1: 0.110761\n",
      "EarlyStopping counter: 7 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "==================== Epoch 37 ====================\n",
      "Running hours: 5.01\n",
      "Train Acc: 0.518930 Train Loss: 2.462324 Train F1: 0.202531\n",
      "  Val Acc: 0.421671   Val Loss: 2.789079 Val F1: 0.202183\n",
      "Validation loss decreased (0.601483 --> 0.578329).  Saving model ...\n",
      "Validation loss decreased (0.836443 --> 0.797817).  Saving model ...\n",
      "==================== Epoch 38 ====================\n",
      "Running hours: 5.14\n",
      "Train Acc: 0.524455 Train Loss: 2.449692 Train F1: 0.206857\n",
      "  Val Acc: 0.387120   Val Loss: 2.892389 Val F1: 0.102663\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 1 out of 10\n",
      "==================== Epoch 39 ====================\n",
      "Running hours: 5.27\n",
      "Train Acc: 0.525683 Train Loss: 2.443741 Train F1: 0.208372\n",
      "  Val Acc: 0.414978   Val Loss: 2.883847 Val F1: 0.192477\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "==================== Epoch 40 ====================\n",
      "Running hours: 5.40\n",
      "Train Acc: 0.530620 Train Loss: 2.427214 Train F1: 0.215713\n",
      "  Val Acc: 0.370658   Val Loss: 2.963630 Val F1: 0.089433\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "==================== Epoch 41 ====================\n",
      "Running hours: 5.54\n",
      "Train Acc: 0.533025 Train Loss: 2.418843 Train F1: 0.213619\n",
      "  Val Acc: 0.438676   Val Loss: 2.728485 Val F1: 0.199745\n",
      "Validation loss decreased (0.578329 --> 0.561324).  Saving model ...\n",
      "EarlyStopping counter: 4 out of 10\n",
      "==================== Epoch 42 ====================\n",
      "Running hours: 5.67\n",
      "Train Acc: 0.536708 Train Loss: 2.402755 Train F1: 0.218011\n",
      "  Val Acc: 0.418234   Val Loss: 2.846577 Val F1: 0.144598\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 5 out of 10\n",
      "==================== Epoch 43 ====================\n",
      "Running hours: 5.80\n",
      "Train Acc: 0.540904 Train Loss: 2.387532 Train F1: 0.223857\n",
      "  Val Acc: 0.464182   Val Loss: 2.649522 Val F1: 0.195099\n",
      "Validation loss decreased (0.561324 --> 0.535818).  Saving model ...\n",
      "EarlyStopping counter: 6 out of 10\n",
      "==================== Epoch 44 ====================\n",
      "Running hours: 5.94\n",
      "Train Acc: 0.543410 Train Loss: 2.387116 Train F1: 0.222401\n",
      "  Val Acc: 0.368849   Val Loss: 3.075822 Val F1: 0.061975\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 7 out of 10\n",
      "==================== Epoch 45 ====================\n",
      "Running hours: 6.07\n",
      "Train Acc: 0.545892 Train Loss: 2.375090 Train F1: 0.224948\n",
      "  Val Acc: 0.433611   Val Loss: 2.768689 Val F1: 0.161739\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 8 out of 10\n",
      "==================== Epoch 46 ====================\n",
      "Running hours: 6.21\n",
      "Train Acc: 0.545687 Train Loss: 2.356568 Train F1: 0.228021\n",
      "  Val Acc: 0.429088   Val Loss: 2.763440 Val F1: 0.156591\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 9 out of 10\n",
      "==================== Epoch 47 ====================\n",
      "Running hours: 6.35\n",
      "Train Acc: 0.553745 Train Loss: 2.343071 Train F1: 0.231854\n",
      "  Val Acc: 0.448987   Val Loss: 2.694226 Val F1: 0.160278\n",
      "EarlyStopping counter: 4 out of 10\n",
      "EarlyStopping counter: 10 out of 10\n",
      "==================== Epoch 48 ====================\n",
      "Running hours: 6.49\n",
      "Train Acc: 0.554538 Train Loss: 2.340472 Train F1: 0.234604\n",
      "  Val Acc: 0.476122   Val Loss: 2.631351 Val F1: 0.180133\n",
      "Validation loss decreased (0.535818 --> 0.523878).  Saving model ...\n",
      "EarlyStopping counter: 11 out of 10\n",
      "==================== Epoch 49 ====================\n",
      "Running hours: 6.63\n",
      "Train Acc: 0.553515 Train Loss: 2.336396 Train F1: 0.236844\n",
      "  Val Acc: 0.463459   Val Loss: 2.617925 Val F1: 0.220378\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.797817 --> 0.779622).  Saving model ...\n",
      "==================== Epoch 50 ====================\n",
      "Running hours: 6.78\n",
      "Train Acc: 0.561547 Train Loss: 2.321750 Train F1: 0.238008\n",
      "  Val Acc: 0.475398   Val Loss: 2.621330 Val F1: 0.192289\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 1 out of 10\n",
      "==================== Epoch 51 ====================\n",
      "Running hours: 6.92\n",
      "Train Acc: 0.565640 Train Loss: 2.305641 Train F1: 0.240779\n",
      "  Val Acc: 0.468343   Val Loss: 2.613349 Val F1: 0.224020\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Validation loss decreased (0.779622 --> 0.775980).  Saving model ...\n",
      "==================== Epoch 52 ====================\n",
      "Running hours: 7.06\n",
      "Train Acc: 0.567175 Train Loss: 2.298120 Train F1: 0.245703\n",
      "  Val Acc: 0.323082   Val Loss: 3.570494 Val F1: 0.180694\n",
      "EarlyStopping counter: 4 out of 10\n",
      "EarlyStopping counter: 1 out of 10\n",
      "==================== Epoch 53 ====================\n",
      "Running hours: 7.20\n",
      "Train Acc: 0.569579 Train Loss: 2.287285 Train F1: 0.248775\n",
      "  Val Acc: 0.431802   Val Loss: 2.769451 Val F1: 0.199815\n",
      "EarlyStopping counter: 5 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "==================== Epoch 54 ====================\n",
      "Running hours: 7.34\n",
      "Train Acc: 0.565717 Train Loss: 2.287161 Train F1: 0.250470\n",
      "  Val Acc: 0.486614   Val Loss: 2.578582 Val F1: 0.215410\n",
      "Validation loss decreased (0.523878 --> 0.513386).  Saving model ...\n",
      "EarlyStopping counter: 3 out of 10\n",
      "==================== Epoch 55 ====================\n",
      "Running hours: 7.48\n",
      "Train Acc: 0.574772 Train Loss: 2.267529 Train F1: 0.252462\n",
      "  Val Acc: 0.440485   Val Loss: 2.752627 Val F1: 0.165639\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "==================== Epoch 56 ====================\n",
      "Running hours: 7.63\n",
      "Train Acc: 0.576461 Train Loss: 2.251510 Train F1: 0.255814\n",
      "  Val Acc: 0.464182   Val Loss: 2.617614 Val F1: 0.196034\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 5 out of 10\n",
      "==================== Epoch 57 ====================\n",
      "Running hours: 7.77\n",
      "Train Acc: 0.580400 Train Loss: 2.246666 Train F1: 0.256192\n",
      "  Val Acc: 0.436143   Val Loss: 2.744136 Val F1: 0.169645\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 6 out of 10\n",
      "==================== Epoch 58 ====================\n",
      "Running hours: 7.91\n",
      "Train Acc: 0.580528 Train Loss: 2.247010 Train F1: 0.257923\n",
      "  Val Acc: 0.448806   Val Loss: 2.640085 Val F1: 0.148954\n",
      "EarlyStopping counter: 4 out of 10\n",
      "EarlyStopping counter: 7 out of 10\n",
      "==================== Epoch 59 ====================\n",
      "Running hours: 8.06\n",
      "Train Acc: 0.583751 Train Loss: 2.231786 Train F1: 0.261027\n",
      "  Val Acc: 0.491498   Val Loss: 2.533848 Val F1: 0.226177\n",
      "Validation loss decreased (0.513386 --> 0.508502).  Saving model ...\n",
      "Validation loss decreased (0.775980 --> 0.773823).  Saving model ...\n",
      "==================== Epoch 60 ====================\n",
      "Running hours: 8.20\n",
      "Train Acc: 0.585849 Train Loss: 2.216556 Train F1: 0.266712\n",
      "  Val Acc: 0.459841   Val Loss: 2.677059 Val F1: 0.165241\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 1 out of 10\n",
      "==================== Epoch 61 ====================\n",
      "Running hours: 8.34\n",
      "Train Acc: 0.588049 Train Loss: 2.216245 Train F1: 0.267779\n",
      "  Val Acc: 0.486795   Val Loss: 2.616390 Val F1: 0.247988\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Validation loss decreased (0.773823 --> 0.752012).  Saving model ...\n",
      "==================== Epoch 62 ====================\n",
      "Running hours: 8.48\n",
      "Train Acc: 0.590428 Train Loss: 2.207589 Train F1: 0.269805\n",
      "  Val Acc: 0.456223   Val Loss: 2.706274 Val F1: 0.164442\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 1 out of 10\n",
      "==================== Epoch 63 ====================\n",
      "Running hours: 8.61\n",
      "Train Acc: 0.592602 Train Loss: 2.199192 Train F1: 0.269486\n",
      "  Val Acc: 0.453871   Val Loss: 2.722484 Val F1: 0.200405\n",
      "EarlyStopping counter: 4 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 100\n",
    "patience = 10\n",
    "log_interval = 1 # print acc and loss in per log_interval time\n",
    "train_acc_list = []\n",
    "train_loss_list = []\n",
    "val_acc_list = []\n",
    "val_loss_list = []\n",
    "train_f1_list = []\n",
    "val_f1_list = []\n",
    "model.load_state_dict(torch.load('../input/best-model-so-far/checkpoint.pt'))\n",
    "early_stopping_cate = EarlyStopping(patience=patience, verbose=True, path='checkpoint_cate.pt')\n",
    "early_stopping_attr = EarlyStopping(patience=patience, verbose=True, path='checkpoint_attr.pt')\n",
    "\n",
    "for epoch in range(1, max_epochs + 1):\n",
    "    train_acc, train_loss, train_f1 = train(train_loader, model, criterion1, criterion2, optimizer)\n",
    "    val_acc, val_loss, val_f1 = val(val_loader, model, criterion1, criterion2)\n",
    "\n",
    "    train_acc_list.append(train_acc)\n",
    "    train_loss_list.append(train_loss)\n",
    "    val_acc_list.append(val_acc)\n",
    "    val_loss_list.append(val_loss)\n",
    "    train_f1_list.append(train_f1)\n",
    "    val_f1_list.append(val_f1)\n",
    "    if epoch % log_interval == 0:\n",
    "        print('=' * 20, 'Epoch', epoch, '=' * 20)\n",
    "        print('Running hours: {:.2f}'.format(running_hours(starttime)))\n",
    "        print('Train Acc: {:.6f} Train Loss: {:.6f} Train F1: {:.6f}'.format(train_acc, train_loss, train_f1))\n",
    "        print('  Val Acc: {:.6f}   Val Loss: {:.6f} Val F1: {:.6f}'.format(val_acc, val_loss, val_f1))\n",
    "        \n",
    "    early_stopping_cate(1-val_acc, model)    \n",
    "    early_stopping_attr(1-val_f1, model)\n",
    "    if (early_stopping_cate.early_stop and early_stopping_attr.early_stop) or running_hours(starttime)>=8.5:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-30T11:54:54.597643Z",
     "iopub.status.busy": "2020-11-30T11:54:54.596965Z",
     "iopub.status.idle": "2020-11-30T11:54:54.601421Z",
     "shell.execute_reply": "2020-11-30T11:54:54.600937Z"
    },
    "papermill": {
     "duration": 0.059246,
     "end_time": "2020-11-30T11:54:54.601526",
     "exception": false,
     "start_time": "2020-11-30T11:54:54.542280",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(input_data, model):\n",
    "    model.load_state_dict(torch.load('./checkpoint_cate.pt'))\n",
    "    model.eval()\n",
    "    output_cate_list = []\n",
    "    output_attr_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in input_data:\n",
    "            images = data[0].cuda()\n",
    "            output_cate, _= model(images)\n",
    "            _, predict_cate = torch.max(output_cate.data, 1)\n",
    "            output_cate_list.extend(predict_cate.to('cpu').numpy().tolist())\n",
    "            \n",
    "    model.load_state_dict(torch.load('./checkpoint_attr.pt'))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in input_data:\n",
    "            images = data[0].cuda()\n",
    "            _, output_attr= model(images)\n",
    "            predict_attr = (output_attr > 0).to('cpu').numpy()\n",
    "            predict_attr_string = [str(np.where(one == True)[0])[1:-1] for one in predict_attr]\n",
    "            output_attr_list.extend(predict_attr_string)\n",
    "            \n",
    "    return output_cate_list, output_attr_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-30T11:54:54.700056Z",
     "iopub.status.busy": "2020-11-30T11:54:54.699245Z",
     "iopub.status.idle": "2020-11-30T11:57:20.477581Z",
     "shell.execute_reply": "2020-11-30T11:57:20.476363Z"
    },
    "papermill": {
     "duration": 145.83088,
     "end_time": "2020-11-30T11:57:20.477711",
     "exception": false,
     "start_time": "2020-11-30T11:54:54.646831",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_cate, output_attr = predict(test_loader, model)\n",
    "path = pd.read_csv(\"../input/deep-fashion/deep_fashion/test.csv\")['file_path']\n",
    "with open('category.csv', 'w', newline='') as csvFile:\n",
    "    writer = csv.DictWriter(csvFile, fieldnames=[\"file_path\",\"category_label\"])\n",
    "    writer.writeheader()\n",
    "    for i in range(len(output_cate)):\n",
    "        writer.writerow({\"file_path\":path.iloc[i], \"category_label\":output_cate[i]})\n",
    "with open('attribute.csv', 'w', newline='') as csvFile:\n",
    "    writer = csv.DictWriter(csvFile, fieldnames=[\"file_path\",\"attribute_label\"])\n",
    "    writer.writeheader()\n",
    "    for i in range(len(output_attr)):\n",
    "        writer.writerow({\"file_path\":path.iloc[i], \"attribute_label\":output_attr[i]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "papermill": {
   "duration": 31148.793622,
   "end_time": "2020-11-30T11:57:22.081561",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-11-30T03:18:13.287939",
   "version": "2.1.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0f16b0ff4e55494eabca6c5b4e49db89": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_223c8086cf4d4fe4b89422658e327b65",
       "placeholder": "​",
       "style": "IPY_MODEL_aa76347415fc480cb16c2a771796b3bd",
       "value": " 97.8M/97.8M [00:23&lt;00:00, 4.46MB/s]"
      }
     },
     "223c8086cf4d4fe4b89422658e327b65": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3b5ec816d32c47d7998828bba94d834c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b0ad9bb0513f4ed599d0fbf424791297",
       "max": 102502400,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e77c188451254035961f3158a0dff2f5",
       "value": 102502400
      }
     },
     "aa76347415fc480cb16c2a771796b3bd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b0ad9bb0513f4ed599d0fbf424791297": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d2f71200596248eb8563a3ff6cf9f2a4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e77c188451254035961f3158a0dff2f5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "f68743efd5a64e9b92a836bd20b983a8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_3b5ec816d32c47d7998828bba94d834c",
        "IPY_MODEL_0f16b0ff4e55494eabca6c5b4e49db89"
       ],
       "layout": "IPY_MODEL_d2f71200596248eb8563a3ff6cf9f2a4"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
